{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "## Méthodologie d’analyse\n",
    "\n",
    "Les étapes suivantes ont été réalisées pour préparer et analyser les données socio-économiques des communes :\n",
    "\n",
    "- **Chargement et nettoyage des données** : Les données brutes issues de l’INSEE ont été importées, puis filtrées pour exclure les communes ou colonnes ne respectant pas les critères de confidentialité ou contenant trop de valeurs manquantes.\n",
    "\n",
    "- **Enrichissement des données** : Un mapping a été effectué pour associer chaque code commune à son libellé, puis une clé unique CITY_ID a été créée pour chaque commune.\n",
    "\n",
    "- **Sélection des variables pertinentes** : Une analyse en composantes principales (PCA) a permis d’identifier les variables ayant le plus d’incidence sur la variabilité des données. Ces variables ont été sélectionnées pour la suite de l’analyse.\n",
    "\n",
    "- **Préparation des features** : Un sous-ensemble de variables d’intérêt a été sélectionné manuellement pour constituer la table de features utilisée dans les analyses de similarité et de clustering.\n",
    "\n",
    "- **Normalisation et réduction de dimension** : Les données ont été normalisées (StandardScaler), puis une réduction de dimension a été appliquée (PCA) pour faciliter les analyses de proximité et de clustering.\n",
    "\n",
    "- **Recherche de villes similaires** : Un algorithme KNN a été utilisé sur les données réduites pour identifier les communes les plus similaires à Levallois-Perret.\n",
    "\n",
    "- **Transformation des données pour l’analyse temporelle** :Les données ont été transformées au format long, puis pivotées pour obtenir un tableau où chaque ligne correspond à une commune et une catégorie, et chaque colonne à une année (2010, 2015, 2021). Une colonne supplémentaire a été calculée pour mesurer l’évolution entre 2015 et 2021.\n",
    "\n",
    "- **Préparation pour l’export et la visualisation** : Les données finales ont été exportées pour une utilisation dans Power BI ou d’autres outils de visualisation.\n",
    "\n",
    "- **Sources** : \n",
    "    - https://www.insee.fr/fr/statistiques/5359146#documentation\n",
    "    - https://www.insee.fr/fr/metadonnees/source/operation/s2146/documentation-methodologique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop for confidentiality (kept from original)\n",
    "CONFIDENTIAL_COLS = ['PIMP21','TP6021','TP60AGE121','TP60AGE221','TP60AGE321','TP60AGE421',\n",
    "                     'TP60AGE521','TP60AGE621','TP60TOL121','TP60TOL221','PACT21','PTSA21',\n",
    "                     'PCHO21','PBEN21','PPEN21','PPAT21','PPSOC21' ,'PPFAM21' ,'PPMINI21',\n",
    "                     'PPLOGT21' ,'PIMPOT21' ,'D121','D921','RD21','ETASSMAT22','ETAUTRES22']\n",
    "\n",
    "YEAR_PREFIX_MAP = {\"P10\": 2010, \"P15\": 2015, \"P21\": 2021}\n",
    "\n",
    "def load_raw(path, sep=';'):\n",
    "    return pd.read_csv(path, sep=sep, dtype=str)\n",
    "\n",
    "def clean_raw(df):\n",
    "    df = df[~df['NBMENFISC21'].isin(['s', 'nd'])]\n",
    "    df = df.drop(columns=[c for c in CONFIDENTIAL_COLS if c in df.columns], errors='ignore')\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def load_meta(meta_path):\n",
    "    meta = pd.read_csv(meta_path, sep=';', dtype=str)\n",
    "    return meta[meta['COD_VAR']=='CODGEO'][['COD_MOD', 'LIB_MOD']]\n",
    "\n",
    "def build_city_df(df, meta_city):\n",
    "    df_city = df.merge(meta_city, left_on='CODGEO', right_on='COD_MOD', how='left')\n",
    "    df_city['CITY_ID'] = df_city['CODGEO'].astype(str) + '_' + df_city['LIB_MOD'].astype(str)\n",
    "    df_city = df_city.drop(['CODGEO', 'COD_MOD','LIB_MOD'], axis=1, errors='ignore')\n",
    "    return df_city.reset_index(drop=True).fillna(0)\n",
    "\n",
    "def select_features_by_prefix(df_city, prefixes=None):\n",
    "    # plus simple à maintenir : prendre colonnes commençant par les préfixes utiles\n",
    "    if prefixes is None:\n",
    "        prefixes = ['P10_','P15_','P21_','C10_','C15_','C21_','ET','P21_POP','P15_POP','P10_POP']\n",
    "    cols = ['CITY_ID'] + [c for c in df_city.columns if any(c.startswith(p) for p in prefixes)]\n",
    "    return df_city[[c for c in cols if c in df_city.columns]]\n",
    "\n",
    "def compute_pca_loadings(df, n_components=None):\n",
    "    X = df.drop(columns=['CITY_ID'])\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=n_components) if n_components else PCA()\n",
    "    pca.fit(Xs)\n",
    "    loadings = pd.DataFrame(pca.components_.T, index=X.columns,\n",
    "                            columns=[f'PC{i+1}' for i in range(pca.components_.shape[0])])\n",
    "    return loadings\n",
    "\n",
    "def knn_similar(df_feature, target_city_id, n_neighbors=25, pca_components=50):\n",
    "    X = df_feature.drop(columns=['CITY_ID']).astype(float)\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=min(pca_components, Xs.shape[1]))\n",
    "    Xp = pca.fit_transform(Xs)\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors+1, metric='euclidean')\n",
    "    knn.fit(Xp)\n",
    "    # find target index\n",
    "    idx = df_feature.index[df_feature['CITY_ID']==target_city_id].tolist()\n",
    "    if not idx:\n",
    "        return []\n",
    "    distances, indices = knn.kneighbors([Xp[idx[0]]])\n",
    "    # skip first (itself)\n",
    "    return df_feature['CITY_ID'].iloc[indices[0]].tolist()\n",
    "\n",
    "def transform_time(df_select_city):\n",
    "    # melt + extract year & category, puis pivot pour analyses temporelles\n",
    "    df_long = df_select_city.melt(id_vars=['CITY_ID'], var_name='column', value_name='value')\n",
    "    df_long['YEAR'] = df_long['column'].str.extract(r'(^P\\d{2})')[0].map(YEAR_PREFIX_MAP)\n",
    "    df_long['CATEGORY'] = df_long['column'].str.extract(r'_(.*)')[0]\n",
    "    df_long = df_long.drop(columns=['column']).dropna(subset=['YEAR','CATEGORY'])\n",
    "    df_long['value'] = df_long['value'].astype(float)\n",
    "    # pivot pour visualisation / export (CITY_ID x YEAR x CATEGORY)\n",
    "    pbi_prep = pd.pivot_table(df_long, values='value', index=['CITY_ID','YEAR'],\n",
    "                              columns='CATEGORY', aggfunc='sum').reset_index()\n",
    "    # also produce df_transformed where CATEGORY are rows and YEAR columns (for evolution calc)\n",
    "    df_transformed = pd.pivot_table(df_long, values='value', index=['CITY_ID','CATEGORY'],\n",
    "                                    columns='YEAR', aggfunc='sum').reset_index()\n",
    "    return df_long.drop_duplicates(), pbi_prep, df_transformed\n",
    "\n",
    "def compute_evolution(df_transformed, y0=2015, y1=2021):\n",
    "    if y0 in df_transformed.columns and y1 in df_transformed.columns:\n",
    "        df_transformed['EVOL_{}_{}'.format(y0%100, y1%100)] = (df_transformed[y1]-df_transformed[y0]) / df_transformed[y0] * 100\n",
    "    return df_transformed\n",
    "\n",
    "def prepare_pbi_export(pbi_df, out_path):\n",
    "    pbi_df.to_csv(out_path, sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = r'G:\\Mon Drive\\Famille Vincent\\Steven\\CESEL\\data\\INSEE\\dossier_complet\\dossier_complet.csv'\n",
    "META_PATH = r'G:\\Mon Drive\\Famille Vincent\\Steven\\CESEL\\data\\INSEE\\dossier_complet\\meta_dossier_complet.csv'\n",
    "PBI_OUT = r''#r'G:\\Mon Drive\\Famille Vincent\\Steven\\CESEL\\data\\pbi_prep.csv'\n",
    "TARGET_CITY = '92044_Levallois-Perret'\n",
    "\n",
    "df = load_raw(RAW_PATH)\n",
    "df = clean_raw(df)\n",
    "meta_city = load_meta(META_PATH)\n",
    "df_city = build_city_df(df, meta_city)\n",
    "\n",
    "df_feature = select_features_by_prefix(df_city)\n",
    "# optional: inspect PCA loadings to refine selection\n",
    "loadings = compute_pca_loadings(df_feature, n_components=10)\n",
    "print(\"PCA loadings preview:\")\n",
    "display(loadings.head())\n",
    "\n",
    "similar_city = knn_similar(df_feature, TARGET_CITY, n_neighbors=25, pca_components=50)\n",
    "print(\"Similar cities (incl. target):\", similar_city)\n",
    "\n",
    "df_select_city = df_city[df_city['CITY_ID'].isin(similar_city) | df_city['CITY_ID'].isin(\n",
    "    ['92009_Bois-Colombes','92026_Courbevoie','92033_Garches','92035_La Garenne-Colombes',\n",
    "     '92051_Neuilly-sur-Seine','92062_Puteaux','92063_Rueil-Malmaison','92073_Suresnes'])]\n",
    "\n",
    "df_long, pbi_prep, df_transformed = transform_time(df_select_city)\n",
    "df_transformed = compute_evolution(df_transformed, 2015, 2021)\n",
    "\n",
    "# export pour PowerBI\n",
    "prepare_pbi_export(pbi_prep, PBI_OUT)\n",
    "\n",
    "# correlation rapide\n",
    "for col in df_feature.columns[1:]:\n",
    "    df_feature[col] = df_feature[col].astype(float)\n",
    "df_pt = df_feature.pivot_table(columns='CITY_ID')\n",
    "leval_rating = df_pt[TARGET_CITY]\n",
    "city_like_levallois = df_pt.corrwith(leval_rating)\n",
    "t = pd.DataFrame(city_like_levallois, columns=['Correlation']).reset_index()\n",
    "display(t.sort_values(by='Correlation', ascending=False).head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
